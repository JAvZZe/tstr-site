
import json
import re
import requests
import uuid

# CONSTANTS
NEW_LISTINGS_FILE = "/home/al/.gemini/antigravity/brain/2382ce1c-5eab-4fa7-97b6-5596e6bbaad6/new_listings_to_add.json"
SQL_OUTPUT_FILE = "insert_new_linkedin_companies.sql"
JSON_OUTPUT_FILE = "insert_new_linkedin_companies.json" # Debug Output
SUPABASE_URL = "https://haimjeaetrsaauitrhfy.supabase.co"
# Using the key found in get_current_listings.py
SUPABASE_KEY = "sb_secret_zRN1fTFOYnN7cEbEIfAP7A_YrEKBfI2" 

headers = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json"
}

def normalize_name(name):
    """Normalize company name for lenient matching."""
    # Lowercase
    n = name.lower()
    # Remove common suffixes
    n = re.sub(r'\b(inc|ltd|llc|gmbh|corp|corporation|limited|company|co)\.?\b', '', n)
    # Remove non-alphanumeric (keep spaces)
    n = re.sub(r'[^a-z0-9\s]', '', n)
    # Collapse whitespace
    n = re.sub(r'\s+', ' ', n).strip()
    return n

def get_db_listings():
    """Fetch all listings (active and pending) to check for duplicates."""
    print("Fetching existing listings from Supabase...")
    all_companies = set()
    
    # 1. Fetch Categories for ID mapping
    cat_map = {}
    try:
        resp_cat = requests.get(f"{SUPABASE_URL}/rest/v1/categories?select=id,slug", headers=headers)
        resp_cat.raise_for_status()
        for c in resp_cat.json():
            cat_map[c['slug']] = c['id']
    except Exception as e:
        print(f"Error fetching categories: {e}")
        return set(), {}

    # 2. Fetch Listings (Active + Pending)
    # We want to check duplicates against EVERYTHING
    page = 0
    limit = 1000
    while True:
        offset = page * limit
        url = f"{SUPABASE_URL}/rest/v1/listings?select=business_name&offset={offset}&limit={limit}"
        try:
            resp = requests.get(url, headers=headers)
            resp.raise_for_status()
            data = resp.json()
            if not data:
                break
            
            for listing in data:
                all_companies.add(normalize_name(listing['business_name']))
            
            if len(data) < limit:
                break
            page += 1
        except Exception as e:
            print(f"Error fetching page {page}: {e}")
            break
            
    print(f"Loaded {len(all_companies)} existing companies for deduplication.")
    return all_companies, cat_map

def generate_sql(new_records, cat_map):
    """Generate SQL insert statements."""
    sql_lines = [
        "-- Bulk insert from LinkedIn Following List",
        "-- Generated by Antigravity",
        "BEGIN;",
        ""
    ]
    
    # Defaults
    default_cat_id = cat_map.get('materials-testing') # Fallback
    
    processed_count = 0
    
    for rec in new_records:
        name = rec['company_name'].replace("'", "''") # Escape SQL
        slug = re.sub(r'[^a-zA-Z0-9]', '-', rec['company_name'].lower())
        slug = re.sub(r'-+', '-', slug).strip('-')
        
        # Unique Slug logic? (Database handles unique constraint, but we should try to be unique)
        # We'll just append random if collision? For now, we assume slug generation is mostly fine.
        
        cat_slug = rec['primary_category']
        cat_id = cat_map.get(cat_slug, default_cat_id)
        
        description = rec['description'].replace("'", "''")
        
        # Location (if available) - parsed raw data has it, but our target schema logic?
        # We'll put location in description or a custom field if schema allows.
        # Check specific schema? 
        # Standard table: id, business_name, slug, description, category_id, status, created_at...
        
        sql = f"""
        INSERT INTO public.listings 
        (business_name, slug, description, category_id, status, is_featured, verified)
        VALUES 
        ('{name}', '{slug}-{str(uuid.uuid4())[:4]}', '{description}', '{cat_id}', 'pending_approval', false, false)
        ON CONFLICT (slug) DO NOTHING;
        """
        sql_lines.append(sql.strip())
        processed_count += 1
        
    sql_lines.append("COMMIT;")
    return "\n".join(sql_lines), processed_count

def main():
    # 1. Load User Data
    try:
        with open(NEW_LISTINGS_FILE, 'r') as f:
            new_data = json.load(f)
    except Exception as e:
        print(f"Error reading {NEW_LISTINGS_FILE}: {e}")
        return

    # 2. Get DB State
    existing_companies, cat_map = get_db_listings()
    
    # 3. Filter
    final_list = []
    ignored_count = 0
    
    for item in new_data:
        norm_name = normalize_name(item['company_name'])
        if norm_name in existing_companies:
            ignored_count += 1
            # print(f"Duplicate found: {item['company_name']}")
        else:
            # Check for internal duplicates in current batch
            # (Simple way: Update existing_companies set as we go)
            if norm_name in existing_companies:
                ignored_count += 1
            else:
                existing_companies.add(norm_name)
                final_list.append(item)
                
    print(f"Processed {len(new_data)} inputs.")
    print(f"Found {ignored_count} duplicates.")
    print(f"New unique companies to add: {len(final_list)}")
    
    # 4. Generate Output
    if final_list:
        # JSON (Backup)
        with open(JSON_OUTPUT_FILE, 'w') as f:
            json.dump(final_list, f, indent=2)
            
        # SQL
        sql_content, count = generate_sql(final_list, cat_map)
        with open(SQL_OUTPUT_FILE, 'w') as f:
            f.write(sql_content)
            
        print(f"\nSUCCESS: Generated SQL script {SQL_OUTPUT_FILE} with {count} inserts.")
        print("Review the SQL before executing.")
        
    else:
        print("No new companies to add.")

if __name__ == "__main__":
    main()
